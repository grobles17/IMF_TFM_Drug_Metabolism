{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b08aa99",
   "metadata": {},
   "source": [
    "# Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8d893f",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22d29426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grobl\\anaconda3\\envs\\IMF-Master\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from numpy.typing import NDArray\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFingerprintGenerator\n",
    "\n",
    "# MolE representations were obtained following the instructions in the original repository:\n",
    "# https://github.com/rolayoalarcon/MolE?tab=readme-ov-file\n",
    "# Using a dedicated conda environment with the specified dependencies, \n",
    "# and running the provided scripts to generate the representations for our dataset. \n",
    "# The resulting TSV file was then read into a DataFrame for analysis.\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7129c06",
   "metadata": {},
   "source": [
    "## 2. Molecular Fingerprints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bc42eb",
   "metadata": {},
   "source": [
    "### Create modifiable generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23a4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Morgan fingerprint generation using RDKit's rdFingerprintGenerator with count-based encoding. ###\n",
    "# Create the generator once at import time\n",
    "def get_morgan_generator(radius: int = 3, n_bits: int = 2048):\n",
    "    return rdFingerprintGenerator.GetMorganGenerator(radius=radius, fpSize=n_bits)\n",
    "\n",
    "# Default global generator (common case)\n",
    "_default_gen = get_morgan_generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b7b03",
   "metadata": {},
   "source": [
    "### Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "376e1c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smiles_to_morgan_fingerprint(smiles: str, generator = _default_gen) -> NDArray[np.int16]:\n",
    "    N_BITS = generator.GetOptions().fpSize\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return np.zeros(N_BITS, dtype=np.int16) \n",
    "    count_fp = generator.GetCountFingerprint(mol)\n",
    "    # Proper conversion from sparse vector to dense array\n",
    "    fp_array = np.zeros(N_BITS, dtype=np.int16)\n",
    "    \n",
    "    # Fill in the non-zero elements\n",
    "    for idx, count_val in count_fp.GetNonzeroElements().items():\n",
    "        fp_array[idx] = count_val\n",
    "    return fp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce8fea",
   "metadata": {},
   "source": [
    "## 3. MolE embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842970dd",
   "metadata": {},
   "source": [
    "Texte explaining the way we got it and the outcome (float64) 1000 dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6886f10",
   "metadata": {},
   "source": [
    "## 4. chemBERTa embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6650248",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1db6d8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 651.62it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mRobertaModel LOAD REPORT\u001b[0m from: seyonec/ChemBERTa-zinc-base-v1\n",
      "Key                       | Status     |  | \n",
      "--------------------------+------------+--+-\n",
      "lm_head.dense.weight      | UNEXPECTED |  | \n",
      "lm_head.decoder.weight    | UNEXPECTED |  | \n",
      "lm_head.dense.bias        | UNEXPECTED |  | \n",
      "lm_head.bias              | UNEXPECTED |  | \n",
      "lm_head.layer_norm.weight | UNEXPECTED |  | \n",
      "lm_head.decoder.bias      | UNEXPECTED |  | \n",
      "lm_head.layer_norm.bias   | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(767, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pre-trained ChemBERTa model and tokenizer once at import time\n",
    "_CHEMBERTA_MODEL_NAME = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "\n",
    "_tokenizer = AutoTokenizer.from_pretrained(_CHEMBERTA_MODEL_NAME)\n",
    "_chemberta_model = AutoModel.from_pretrained(_CHEMBERTA_MODEL_NAME)\n",
    "_chemberta_model.eval()  # disable dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859b5f30",
   "metadata": {},
   "source": [
    "Explain wtf is this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bede8afa",
   "metadata": {},
   "source": [
    "### Def function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce99e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chemberta_embedder(smiles: str) -> NDArray[np.float32]:\n",
    "    inputs = _tokenizer(\n",
    "        smiles,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=1024 #vancomycin's SMILES has more than 700 tokens, \n",
    "        #so we set a higher limit to avoid truncation for large molecules\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = _chemberta_model(**inputs)\n",
    "\n",
    "    # Mean pooling over token embeddings (excluding padding)\n",
    "    last_hidden = outputs.last_hidden_state  # (1, seq_len, hidden_dim)\n",
    "    attention_mask = inputs[\"attention_mask\"].unsqueeze(-1)\n",
    "\n",
    "    masked_hidden = last_hidden * attention_mask\n",
    "    sum_hidden = masked_hidden.sum(dim=1)\n",
    "    valid_tokens = attention_mask.sum(dim=1)\n",
    "\n",
    "    embedding = sum_hidden / valid_tokens\n",
    "\n",
    "    return embedding.squeeze(0).cpu().numpy().astype(np.float64) #float 64 is used for compatibility with MolE embeddings, which are also float64.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2615d1e",
   "metadata": {},
   "source": [
    "## 5. Harmonization in one function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15477ccf",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def featurize_smiles(smiles: str, method: str = \"morgan\") -> NDArray[np.int16 | np.float64]:\n",
    "    if method == \"morgan\":\n",
    "        return smiles_to_morgan_fingerprint(smiles).astype(np.int16)\n",
    "    elif method == \"chemberta\":\n",
    "        return chemberta_embedder(smiles).astype(np.float64)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown featurization method: {method}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bfe09",
   "metadata": {},
   "source": [
    "## 6. function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5bb4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv(\"DrugBank_curated_df.csv\")\n",
    "\n",
    "ids = []\n",
    "fingerprints = []\n",
    "chemberta_embeddings = []\n",
    "\n",
    "for compound_id, smiles in zip(db[\"DrugBank ID\"], db[\"SMILES\"]):\n",
    "    fp = featurize_smiles(smiles, method=\"morgan\")\n",
    "    emb = featurize_smiles(smiles, method=\"chemberta\")\n",
    "    ids.append(compound_id)\n",
    "    fingerprints.append(fp)\n",
    "    chemberta_embeddings.append(emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077efb33",
   "metadata": {},
   "source": [
    "## 7. Data formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b03b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to 2D numpy array\n",
    "fingerprint_matrix = np.vstack(fingerprints)\n",
    "chemberta_matrix = np.vstack(chemberta_embeddings)\n",
    "\n",
    "    # Create DataFrame with explicit column names\n",
    "morgan_df = pd.DataFrame(\n",
    "    fingerprint_matrix,\n",
    "    index=ids,\n",
    "    columns=[str(i) for i in range(fingerprint_matrix.shape[1])]\n",
    ")\n",
    "chemberta_df = pd.DataFrame(\n",
    "    chemberta_matrix,\n",
    "    index=ids,\n",
    "    columns=[str(i) for i in range(chemberta_matrix.shape[1])]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36396d14",
   "metadata": {},
   "source": [
    "## 8. Data comprobations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25806e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mole_df = pd.read_csv(\"MolE_output_representation.tsv\", sep=\"\\t\", index_col=0)\n",
    "print(\"Shape of the MolE representation DataFrame:\")\n",
    "print(mole_df.shape)\n",
    "print(\"Shape of the Morgan fingerprint DataFrame:\")\n",
    "print(morgan_df.shape)\n",
    "print(round(morgan_df.memory_usage(deep=True).sum() / 1024**2, 2), \"MB\")\n",
    "print(\"Shape of the ChemBERTa embedding DataFrame:\")\n",
    "print(chemberta_df.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IMF-Master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
